{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/content/LLaMA-Factory/'\n",
      "d:\\AI_CODE\\MeOracle_Model\\NLP_Model\\LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-08-04 09:00:03,620 >> loading file vocab.json from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\vocab.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-04 09:00:03,620 >> loading file merges.txt from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\merges.txt\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-04 09:00:03,620 >> loading file tokenizer.json from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-04 09:00:03,620 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-04 09:00:03,623 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-04 09:00:03,624 >> loading file tokenizer_config.json from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-08-04 09:00:04,004 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/04/2024 09:00:04 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-08-04 09:00:04,258 >> loading configuration file config.json from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-08-04 09:00:04,262 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-0.5B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/04/2024 09:00:04 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
      "08/04/2024 09:00:04 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3474] 2024-08-04 09:00:04,436 >> loading weights file model.safetensors from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-08-04 09:00:04,514 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-08-04 09:00:04,518 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-08-04 09:00:07,822 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-08-04 09:00:07,822 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-08-04 09:00:08,100 >> loading configuration file generation_config.json from cache at C:\\Users\\tridu\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-0.5B-Chat\\snapshots\\4d14e384a4b037942bb3f3016665157c8bcb70ea\\generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-08-04 09:00:08,101 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/04/2024 09:00:08 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/04/2024 09:00:09 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen1.5-0.5B-Chat/lora/qwen_full_data\n",
      "08/04/2024 09:00:09 - INFO - llamafactory.model.loader - all params: 467,772,416\n",
      "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
      "Assistant: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tridu\\miniconda3\\envs\\d2l\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:679: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headache\n",
      "Assistant: panic disorder"
     ]
    }
   ],
   "source": [
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "%cd /content/LLaMA-Factory/\n",
    "# args = dict(\n",
    "#   model_name_or_path=\"THUDM/glm-4-9b\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "#   adapter_name_or_path=\"saves/GLM-4-9B/lora/train_multilabel\",            # load the saved LoRA adapters\n",
    "#   template=\"default\",                     # same to the one in training\n",
    "#   finetuning_type=\"lora\",                  # same to the one in training\n",
    "#   quantization_bit=4,                    # load 4-bit quantized model\n",
    "# )\n",
    "\n",
    "# args = dict(\n",
    "#   model_name_or_path=\"bigscience/bloom-560m\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "#   adapter_name_or_path=\"saves/BLOOM-560M/lora/bloom_full_\",            # load the saved LoRA adapters\n",
    "#   template=\"default\",                     # same to the one in training\n",
    "#   finetuning_type=\"lora\",                  # same to the one in training\n",
    "#   quantization_bit=4,                    # load 4-bit quantized model\n",
    "# )\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\"Qwen/Qwen1.5-0.5B-Chat\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=\"saves/Qwen1.5-0.5B-Chat/lora/qwen_full_data\",            # load the saved LoRA adapters\n",
    "  template=\"qwen\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")\n",
    "\n",
    "# args = dict(\n",
    "#   model_name_or_path=\"google/gemma-2b\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "#   adapter_name_or_path=\"saves/Gemma-2B/lora/Gemma_2B_fix\",            # load the saved LoRA adapters\n",
    "#   template=\"default\",                     # same to the one in training\n",
    "#   finetuning_type=\"lora\",                  # same to the one in training\n",
    "#   quantization_bit=4,                    # load 4-bit quantized model\n",
    "# )\n",
    "chat_model = ChatModel(args)\n",
    "\n",
    "messages = []\n",
    "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"History has been removed.\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "torch_gc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
