{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "NAME = \"Llama-3\"\n",
    "AUTHOR = \"LLaMA Factory\"\n",
    "\n",
    "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "for sample in dataset:\n",
    "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
    "\n",
    "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 22:55:52.833237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 22:55:52.846124: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 22:55:52.850072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-29 22:55:52.859156: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 22:55:53.440565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "usage: train.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                [--adapter_name_or_path ADAPTER_NAME_OR_PATH]\n",
      "                [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR]\n",
      "                [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
      "                [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]]\n",
      "                [--split_special_tokens [SPLIT_SPECIAL_TOKENS]]\n",
      "                [--new_special_tokens NEW_SPECIAL_TOKENS]\n",
      "                [--model_revision MODEL_REVISION]\n",
      "                [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]\n",
      "                [--no_low_cpu_mem_usage]\n",
      "                [--quantization_method {bitsandbytes,hqq,eetq}]\n",
      "                [--quantization_bit QUANTIZATION_BIT]\n",
      "                [--quantization_type {fp4,nf4}]\n",
      "                [--double_quantization [DOUBLE_QUANTIZATION]]\n",
      "                [--no_double_quantization] [--quantization_device_map {auto}]\n",
      "                [--rope_scaling {linear,dynamic}]\n",
      "                [--flash_attn {auto,disabled,sdpa,fa2}]\n",
      "                [--shift_attn [SHIFT_ATTN]]\n",
      "                [--mixture_of_depths {convert,load}]\n",
      "                [--use_unsloth [USE_UNSLOTH]]\n",
      "                [--visual_inputs [VISUAL_INPUTS]]\n",
      "                [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]\n",
      "                [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]\n",
      "                [--upcast_layernorm [UPCAST_LAYERNORM]]\n",
      "                [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]\n",
      "                [--train_from_scratch [TRAIN_FROM_SCRATCH]]\n",
      "                [--infer_backend {huggingface,vllm}]\n",
      "                [--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL]\n",
      "                [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]\n",
      "                [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]\n",
      "                [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]]\n",
      "                [--no_use_cache]\n",
      "                [--infer_dtype {auto,float16,bfloat16,float32}]\n",
      "                [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]\n",
      "                [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE]\n",
      "                [--export_device {cpu,auto}]\n",
      "                [--export_quantization_bit EXPORT_QUANTIZATION_BIT]\n",
      "                [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]\n",
      "                [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]\n",
      "                [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]\n",
      "                [--export_legacy_format [EXPORT_LEGACY_FORMAT]]\n",
      "                [--export_hub_model_id EXPORT_HUB_MODEL_ID]\n",
      "                [--print_param_status [PRINT_PARAM_STATUS]]\n",
      "                [--template TEMPLATE] [--dataset DATASET]\n",
      "                [--eval_dataset EVAL_DATASET] [--dataset_dir DATASET_DIR]\n",
      "                [--cutoff_len CUTOFF_LEN]\n",
      "                [--train_on_prompt [TRAIN_ON_PROMPT]]\n",
      "                [--mask_history [MASK_HISTORY]] [--streaming [STREAMING]]\n",
      "                [--buffer_size BUFFER_SIZE]\n",
      "                [--mix_strategy {concat,interleave_under,interleave_over}]\n",
      "                [--interleave_probs INTERLEAVE_PROBS]\n",
      "                [--overwrite_cache [OVERWRITE_CACHE]]\n",
      "                [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
      "                [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]\n",
      "                [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]\n",
      "                [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]\n",
      "                [--packing PACKING] [--neat_packing [NEAT_PACKING]]\n",
      "                [--tool_format TOOL_FORMAT] [--tokenized_path TOKENIZED_PATH]\n",
      "                --output_dir OUTPUT_DIR\n",
      "                [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                [--do_predict [DO_PREDICT]] [--eval_strategy {no,steps,epoch}]\n",
      "                [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                [--eval_delay EVAL_DELAY]\n",
      "                [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]\n",
      "                [--learning_rate LEARNING_RATE] [--weight_decay WEIGHT_DECAY]\n",
      "                [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]\n",
      "                [--adam_epsilon ADAM_EPSILON] [--max_grad_norm MAX_GRAD_NORM]\n",
      "                [--num_train_epochs NUM_TRAIN_EPOCHS] [--max_steps MAX_STEPS]\n",
      "                [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]\n",
      "                [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "                [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
      "                [--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "                [--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "                [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "                [--no_log_on_each_node] [--logging_dir LOGGING_DIR]\n",
      "                [--logging_strategy {no,steps,epoch}]\n",
      "                [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                [--logging_steps LOGGING_STEPS]\n",
      "                [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "                [--no_logging_nan_inf_filter]\n",
      "                [--save_strategy {no,steps,epoch}] [--save_steps SAVE_STEPS]\n",
      "                [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                [--save_safetensors [SAVE_SAFETENSORS]]\n",
      "                [--no_save_safetensors]\n",
      "                [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "                [--save_only_model [SAVE_ONLY_MODEL]]\n",
      "                [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
      "                [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "                [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "                [--data_seed DATA_SEED] [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "                [--use_ipex [USE_IPEX]] [--bf16 [BF16]] [--fp16 [FP16]]\n",
      "                [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                [--half_precision_backend {auto,apex,cpu_amp}]\n",
      "                [--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "                [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "                [--local_rank LOCAL_RANK]\n",
      "                [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl}]\n",
      "                [--tpu_num_cores TPU_NUM_CORES]\n",
      "                [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                [--debug DEBUG [DEBUG ...]]\n",
      "                [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                [--eval_steps EVAL_STEPS]\n",
      "                [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "                [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                [--disable_tqdm DISABLE_TQDM]\n",
      "                [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                [--no_remove_unused_columns]\n",
      "                [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                [--greater_is_better GREATER_IS_BETTER]\n",
      "                [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]\n",
      "                [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "                [--fsdp_config FSDP_CONFIG]\n",
      "                [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "                [--accelerator_config ACCELERATOR_CONFIG]\n",
      "                [--deepspeed DEEPSPEED]\n",
      "                [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise,lomo,adalomo}]\n",
      "                [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]\n",
      "                [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                [--report_to REPORT_TO]\n",
      "                [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "                [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "                [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                [--no_dataloader_pin_memory]\n",
      "                [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "                [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                [--no_skip_memory_metrics]\n",
      "                [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "                [--push_to_hub [PUSH_TO_HUB]]\n",
      "                [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                [--hub_model_id HUB_MODEL_ID]\n",
      "                [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "                [--hub_token HUB_TOKEN]\n",
      "                [--hub_private_repo [HUB_PRIVATE_REPO]]\n",
      "                [--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "                [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "                [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "                [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "                [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
      "                [--no_eval_do_concat_batches]\n",
      "                [--fp16_backend {auto,apex,cpu_amp}]\n",
      "                [--evaluation_strategy {no,steps,epoch}]\n",
      "                [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "                [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "                [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "                [--mp_parameters MP_PARAMETERS]\n",
      "                [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "                [--full_determinism [FULL_DETERMINISM]]\n",
      "                [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]\n",
      "                [--ddp_timeout DDP_TIMEOUT] [--torch_compile [TORCH_COMPILE]]\n",
      "                [--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "                [--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "                [--dispatch_batches DISPATCH_BATCHES]\n",
      "                [--split_batches SPLIT_BATCHES]\n",
      "                [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "                [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "                [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "                [--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "                [--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
      "                [--eval_on_start [EVAL_ON_START]]\n",
      "                [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]\n",
      "                [--sortish_sampler [SORTISH_SAMPLER]]\n",
      "                [--predict_with_generate [PREDICT_WITH_GENERATE]]\n",
      "                [--generation_max_length GENERATION_MAX_LENGTH]\n",
      "                [--generation_num_beams GENERATION_NUM_BEAMS]\n",
      "                [--generation_config GENERATION_CONFIG]\n",
      "                [--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]\n",
      "                [--badam_start_block BADAM_START_BLOCK]\n",
      "                [--badam_switch_mode {ascending,descending,random,fixed}]\n",
      "                [--badam_switch_interval BADAM_SWITCH_INTERVAL]\n",
      "                [--badam_update_ratio BADAM_UPDATE_RATIO]\n",
      "                [--badam_mask_mode {adjacent,scatter}]\n",
      "                [--badam_verbose BADAM_VERBOSE] [--use_galore [USE_GALORE]]\n",
      "                [--galore_target GALORE_TARGET] [--galore_rank GALORE_RANK]\n",
      "                [--galore_update_interval GALORE_UPDATE_INTERVAL]\n",
      "                [--galore_scale GALORE_SCALE]\n",
      "                [--galore_proj_type {std,reverse_std,right,left,full}]\n",
      "                [--galore_layerwise [GALORE_LAYERWISE]]\n",
      "                [--pref_beta PREF_BETA] [--pref_ftx PREF_FTX]\n",
      "                [--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]\n",
      "                [--dpo_label_smoothing DPO_LABEL_SMOOTHING]\n",
      "                [--kto_chosen_weight KTO_CHOSEN_WEIGHT]\n",
      "                [--kto_rejected_weight KTO_REJECTED_WEIGHT]\n",
      "                [--simpo_gamma SIMPO_GAMMA]\n",
      "                [--ppo_buffer_size PPO_BUFFER_SIZE] [--ppo_epochs PPO_EPOCHS]\n",
      "                [--ppo_score_norm [PPO_SCORE_NORM]] [--ppo_target PPO_TARGET]\n",
      "                [--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]\n",
      "                [--ref_model REF_MODEL]\n",
      "                [--ref_model_adapters REF_MODEL_ADAPTERS]\n",
      "                [--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]\n",
      "                [--reward_model REWARD_MODEL]\n",
      "                [--reward_model_adapters REWARD_MODEL_ADAPTERS]\n",
      "                [--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]\n",
      "                [--reward_model_type {lora,full,api}]\n",
      "                [--additional_target ADDITIONAL_TARGET]\n",
      "                [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]\n",
      "                [--lora_rank LORA_RANK] [--lora_target LORA_TARGET]\n",
      "                [--loraplus_lr_ratio LORAPLUS_LR_RATIO]\n",
      "                [--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]\n",
      "                [--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]\n",
      "                [--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]\n",
      "                [--pissa_convert [PISSA_CONVERT]]\n",
      "                [--create_new_adapter [CREATE_NEW_ADAPTER]]\n",
      "                [--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]\n",
      "                [--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]\n",
      "                [--freeze_extra_modules FREEZE_EXTRA_MODULES]\n",
      "                [--pure_bf16 [PURE_BF16]] [--stage {pt,sft,rm,ppo,dpo,kto}]\n",
      "                [--finetuning_type {lora,freeze,full}]\n",
      "                [--use_llama_pro [USE_LLAMA_PRO]]\n",
      "                [--freeze_vision_tower [FREEZE_VISION_TOWER]]\n",
      "                [--no_freeze_vision_tower]\n",
      "                [--train_mm_proj_only [TRAIN_MM_PROJ_ONLY]]\n",
      "                [--compute_accuracy [COMPUTE_ACCURACY]]\n",
      "                [--plot_loss [PLOT_LOSS]] [--do_sample [DO_SAMPLE]]\n",
      "                [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P]\n",
      "                [--top_k TOP_K] [--num_beams NUM_BEAMS]\n",
      "                [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]\n",
      "                [--repetition_penalty REPETITION_PENALTY]\n",
      "                [--length_penalty LENGTH_PENALTY]\n",
      "                [--default_system DEFAULT_SYSTEM]\n",
      "train.py: error: the following arguments are required: --model_name_or_path, --output_dir\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python src/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/LLaMA-Factory/'\n",
      "/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-30 12:00:40.221517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:00:40.234512: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:00:40.238465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:00:40.247411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:00:40.828143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://4bb736a1bf5c45e884.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "2024-07-30 12:06:11.242519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:06:11.255383: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:06:11.259330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:06:11.268255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:06:11.850234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "07/30/2024 12:06:15 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23243\n",
      "W0730 12:06:16.099000 139817514500160 torch/distributed/run.py:757] \n",
      "W0730 12:06:16.099000 139817514500160 torch/distributed/run.py:757] *****************************************\n",
      "W0730 12:06:16.099000 139817514500160 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0730 12:06:16.099000 139817514500160 torch/distributed/run.py:757] *****************************************\n",
      "2024-07-30 12:06:17.709296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:06:17.713445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:06:17.722156: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:06:17.726029: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:06:17.726038: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:06:17.729875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:06:17.735038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:06:17.738798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:06:18.309948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-30 12:06:18.320911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "07/30/2024 12:06:21 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "07/30/2024 12:06:21 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "07/30/2024 12:06:21 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "07/30/2024 12:06:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_auto.py:682] 2024-07-30 12:06:22,098 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "[rank1]:     response.raise_for_status()\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank1]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank1]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
      "[rank1]:     resolved_file = hf_hub_download(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank1]:     return f(*args, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
      "[rank1]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n",
      "[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank1]:     raise head_call_error\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1751, in _get_metadata_or_catch_error\n",
      "[rank1]:     metadata = get_hf_file_metadata(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1673, in get_hf_file_metadata\n",
      "[rank1]:     r = _request_wrapper(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n",
      "[rank1]:     response = _request_wrapper(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n",
      "[rank1]:     hf_raise_for_status(response)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
      "[rank1]:     raise GatedRepoError(message, response) from e\n",
      "[rank1]: huggingface_hub.utils._errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66a874ce-5b300dcb55fec9ca05b968fd;a852a735-d0f6-4c28-85d9-9b0cbb202a70)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "[rank1]: Access to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank1]:     launch()\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank1]:     run_exp()\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 44, in run_sft\n",
      "[rank1]:     tokenizer_module = load_tokenizer(model_args)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
      "[rank1]:     tokenizer = AutoTokenizer.from_pretrained(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 853, in from_pretrained\n",
      "[rank1]:     config = AutoConfig.from_pretrained(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 972, in from_pretrained\n",
      "[rank1]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
      "[rank1]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
      "[rank1]:     resolved_config_file = cached_file(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 420, in cached_file\n",
      "[rank1]:     raise EnvironmentError(\n",
      "[rank1]: OSError: You are trying to access a gated repo.\n",
      "[rank1]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n",
      "[rank1]: 403 Client Error. (Request ID: Root=1-66a874ce-5b300dcb55fec9ca05b968fd;a852a735-d0f6-4c28-85d9-9b0cbb202a70)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "[rank1]: Access to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "[rank0]:     response.raise_for_status()\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank0]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank0]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
      "[rank0]:     resolved_file = hf_hub_download(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank0]:     return f(*args, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
      "[rank0]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n",
      "[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank0]:     raise head_call_error\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1751, in _get_metadata_or_catch_error\n",
      "[rank0]:     metadata = get_hf_file_metadata(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1673, in get_hf_file_metadata\n",
      "[rank0]:     r = _request_wrapper(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n",
      "[rank0]:     response = _request_wrapper(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n",
      "[rank0]:     hf_raise_for_status(response)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
      "[rank0]:     raise GatedRepoError(message, response) from e\n",
      "[rank0]: huggingface_hub.utils._errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66a874ce-7065c73b2d308aeb66caa0b8;4cad42ba-aa1c-4b21-bf07-ba5c30f87b19)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "[rank0]: Access to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 44, in run_sft\n",
      "[rank0]:     tokenizer_module = load_tokenizer(model_args)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
      "[rank0]:     tokenizer = AutoTokenizer.from_pretrained(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 853, in from_pretrained\n",
      "[rank0]:     config = AutoConfig.from_pretrained(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 972, in from_pretrained\n",
      "[rank0]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
      "[rank0]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
      "[rank0]:     resolved_config_file = cached_file(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 420, in cached_file\n",
      "[rank0]:     raise EnvironmentError(\n",
      "[rank0]: OSError: You are trying to access a gated repo.\n",
      "[rank0]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n",
      "[rank0]: 403 Client Error. (Request ID: Root=1-66a874ce-7065c73b2d308aeb66caa0b8;4cad42ba-aa1c-4b21-bf07-ba5c30f87b19)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "[rank0]: Access to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.\n",
      "E0730 12:06:26.111000 139817514500160 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 2101622) of binary: /media/mountHDD2/zeus/.env/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/mountHDD2/zeus/.env/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "    run(args)\n",
      "  File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "    elastic_launch(\n",
      "  File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-07-30_12:06:26\n",
      "  host      : exx-desktop\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2101623)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-07-30_12:06:26\n",
      "  host      : exx-desktop\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 2101622)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "2024-07-30 12:07:28.764565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:07:28.777228: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:07:28.781120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:07:28.789963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:07:29.365194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "07/30/2024 12:07:32 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22943\n",
      "W0730 12:07:33.451000 140022109708352 torch/distributed/run.py:757] \n",
      "W0730 12:07:33.451000 140022109708352 torch/distributed/run.py:757] *****************************************\n",
      "W0730 12:07:33.451000 140022109708352 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0730 12:07:33.451000 140022109708352 torch/distributed/run.py:757] *****************************************\n",
      "2024-07-30 12:07:35.046512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:07:35.058950: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:07:35.062763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:07:35.071542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:07:35.090894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 12:07:35.103392: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 12:07:35.107214: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 12:07:35.116010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 12:07:35.659129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-30 12:07:35.694137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "07/30/2024 12:07:39 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "07/30/2024 12:07:39 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "07/30/2024 12:07:39 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "07/30/2024 12:07:39 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-07-30 12:07:39,762 >> loading file tokenizer.model from cache at /media/mountHDD2/zeus/.cache/huggingface/hub/models--THUDM--glm-4-9b/snapshots/5dd1ab74bb3717ceb6acc17fedbdcd17028ce841/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-07-30 12:07:39,762 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-07-30 12:07:39,762 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-07-30 12:07:39,762 >> loading file tokenizer_config.json from cache at /media/mountHDD2/zeus/.cache/huggingface/hub/models--THUDM--glm-4-9b/snapshots/5dd1ab74bb3717ceb6acc17fedbdcd17028ce841/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-07-30 12:07:39,762 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2533] 2024-07-30 12:07:40,056 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/30/2024 12:07:40 - INFO - llamafactory.data.loader - Loading dataset fhai50032/SymptomsDisease246k...\n",
      "Converting format of dataset (num_proc=16): 100%|█| 493890/493890 [00:00<00:00, \n",
      "07/30/2024 12:07:46 - INFO - llamafactory.data.loader - Loading dataset fhai50032/SymptomsDisease246k...\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 493890/493890 [00:19<00:00, \n",
      "training example:\n",
      "input_ids:\n",
      "[33811, 25, 18028, 323, 22537, 2090, 1154, 8675, 2090, 315, 11481, 1154, 450, 67965, 476, 92360, 13486, 1154, 26237, 10291, 2090, 1154, 19272, 79, 30547, 1154, 404, 22251, 51764, 1154, 20469, 43312, 4937, 715, 71157, 25, 19044, 19231, 151329]\n",
      "inputs:\n",
      "Human: anxiety and nervousness ,shortness of breath ,depressive or psychotic symptoms ,chest tightness ,palpitations ,irregular heartbeat ,breathing fast \n",
      "Assistant:panic disorder <|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19044, 19231, 151329]\n",
      "labels:\n",
      "panic disorder <|endoftext|>\n",
      "[INFO|configuration_utils.py:733] 2024-07-30 12:08:07,410 >> loading configuration file config.json from cache at /media/mountHDD2/zeus/.cache/huggingface/hub/models--THUDM--glm-4-9b/snapshots/5dd1ab74bb3717ceb6acc17fedbdcd17028ce841/config.json\n",
      "[INFO|configuration_utils.py:733] 2024-07-30 12:08:07,919 >> loading configuration file config.json from cache at /media/mountHDD2/zeus/.cache/huggingface/hub/models--THUDM--glm-4-9b/snapshots/5dd1ab74bb3717ceb6acc17fedbdcd17028ce841/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-30 12:08:07,920 >> Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/glm-4-9b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/glm-4-9b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/glm-4-9b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/glm-4-9b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/glm-4-9b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/glm-4-9b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": [\n",
      "    151329,\n",
      "    151336,\n",
      "    151338\n",
      "  ],\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1.5625e-07,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 40,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 151329,\n",
      "  \"padded_vocab_size\": 151552,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"rmsnorm\": true,\n",
      "  \"rope_ratio\": 1,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 151552\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3634] 2024-07-30 12:08:08,214 >> loading weights file model.safetensors from cache at /media/mountHDD2/zeus/.cache/huggingface/hub/models--THUDM--glm-4-9b/snapshots/5dd1ab74bb3717ceb6acc17fedbdcd17028ce841/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                | 0/10 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|                                | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:   7%|▎    | 136M/1.95G [00:08<24:33, 1.23MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:   8%|▍    | 147M/1.95G [00:17<24:48, 1.21MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:   8%|▍    | 157M/1.95G [00:26<24:47, 1.20MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:   9%|▍    | 168M/1.95G [00:34<24:41, 1.20MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:   9%|▍    | 178M/1.95G [00:43<24:45, 1.19MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:  10%|▍    | 189M/1.95G [00:52<24:24, 1.20MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:  10%|▌    | 199M/1.95G [01:01<24:17, 1.20MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:  11%|▌    | 210M/1.95G [01:09<24:09, 1.20MB/s]\u001b[A\n",
      "model-00001-of-00010.safetensors:  11%|▌    | 220M/1.95G [01:18<24:02, 1.20MB/s]\u001b[A^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "W0730 12:09:34.716000 140022109708352 torch/distributed/elastic/agent/server/api.py:741] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0730 12:09:34.717000 140022109708352 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2102060 closing signal SIGINT\n",
      "W0730 12:09:34.717000 140022109708352 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2102061 closing signal SIGINT\n",
      "Downloading shards:   0%|                                | 0/10 [01:25<?, ?it/s]\n",
      "model-00001-of-00010.safetensors:  11%|▌    | 220M/1.95G [01:25<26:09, 1.10MB/s]\n",
      "Downloading shards:   0%|                                | 0/10 [01:26<?, ?it/s]\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank1]:     launch()\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank1]:     run_exp()\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "[rank1]:     model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/model/loader.py\", line 153, in load_model\n",
      "[rank1]:     model = AutoModelForCausalLM.from_pretrained(**init_kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n",
      "[rank1]:     return model_class.from_pretrained(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3671, in from_pretrained\n",
      "[rank1]:     resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 1079, in get_checkpoint_shard_files\n",
      "[rank1]:     cached_filename = cached_file(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
      "[rank1]:     resolved_file = hf_hub_download(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank1]:     return f(*args, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
      "[rank1]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1388, in _hf_hub_download_to_cache_dir\n",
      "[rank1]:     with WeakFileLock(lock_path):\n",
      "[rank1]:   File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n",
      "[rank1]:     return next(self.gen)\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_fixes.py\", line 91, in WeakFileLock\n",
      "[rank1]:     lock.acquire()\n",
      "[rank1]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/filelock/_api.py\", line 267, in acquire\n",
      "[rank1]:     time.sleep(poll_interval)\n",
      "[rank1]: KeyboardInterrupt\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "[rank0]:     model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/project/MeOracle/MeOracle_Model/NLP_Model/LLaMA-Factory/src/llamafactory/model/loader.py\", line 153, in load_model\n",
      "[rank0]:     model = AutoModelForCausalLM.from_pretrained(**init_kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n",
      "[rank0]:     return model_class.from_pretrained(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3671, in from_pretrained\n",
      "[rank0]:     resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 1079, in get_checkpoint_shard_files\n",
      "[rank0]:     cached_filename = cached_file(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
      "[rank0]:     resolved_file = hf_hub_download(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank0]:     return f(*args, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
      "[rank0]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n",
      "[rank0]:     _download_to_tmp_and_move(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n",
      "[rank0]:     http_get(\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 549, in http_get\n",
      "[rank0]:     for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n",
      "[rank0]:     yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/urllib3/response.py\", line 1043, in stream\n",
      "[rank0]:     data = self.read(amt=amt, decode_content=decode_content)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/urllib3/response.py\", line 935, in read\n",
      "[rank0]:     data = self._raw_read(amt)\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/urllib3/response.py\", line 862, in _raw_read\n",
      "[rank0]:     data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "[rank0]:   File \"/media/mountHDD2/zeus/.env/lib/python3.10/site-packages/urllib3/response.py\", line 845, in _fp_read\n",
      "[rank0]:     return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "[rank0]:   File \"/usr/lib/python3.10/http/client.py\", line 465, in read\n",
      "[rank0]:     s = self.fp.read(amt)\n",
      "[rank0]:   File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "[rank0]:     return self._sock.recv_into(b)\n",
      "[rank0]:   File \"/usr/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
      "[rank0]:     return self.read(nbytes, buffer)\n",
      "[rank0]:   File \"/usr/lib/python3.10/ssl.py\", line 1130, in read\n",
      "[rank0]:     return self._sslobj.read(len, buffer)\n",
      "[rank0]: KeyboardInterrupt\n",
      "Killing tunnel 0.0.0.0:7860 <> https://4bb736a1bf5c45e884.gradio.live\n"
     ]
    }
   ],
   "source": [
    "%cd /content/LLaMA-Factory/\n",
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
